{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc #garbage collect\nimport random as rd # generating random numbers\nimport datetime # manipulating date formats\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom multiprocessing import Pool\nfrom itertools import product\nfrom sklearn import preprocessing\nimport xgboost as xgb\nInteractiveShell.ast_node_interactivity = \"all\"\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns \nimport lightgbm as lgb\nfrom sklearn import feature_extraction\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm, tqdm_notebook\n\n%matplotlib inline\n\n# Function to downcast data types and reduce memory consumption\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\nimport os\nprint(os.listdir(\"../input\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4979725ed09f0c6fb8d35f2bca3380284fd6cefa"
      },
      "cell_type": "markdown",
      "source": "**EDA** -\nHere is a summary of the EDA and the insights thus derived performed on the datasets. The code here summarizes some of the key data analyses I performed and how it was used to create features. The idea here was to look for trends in data and given that it was a time series problem, the intention was to check whether the problem would need to be solved by a time-series model (ARIMA etc.) or whether it can be solved using traditonal statistical methods"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Import datasets\ntrain=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nitem_cats = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n\n\n#Merge datasets, convert to datetime format, calculate total sales\ntrain = pd.merge(train,shops,on='shop_id',how='left')\ntrain = pd.merge(train,items,on='item_id',how='left')\ntrain = pd.merge(train,item_cats,on='item_category_id',how='left')\ntrain['date_format'] = pd.to_datetime(train.date,format='%d.%m.%Y')\ntrain['total_sales']=train['item_price']*train['item_cnt_day']\n\n# Convert datetime to date, month, year and weekday columns for EDA\ndef get_year(x):\n    return x.year\ndef get_month(x):\n    return x.month\ndef get_day(x):\n    return x.day\ndef get_weekday(x):\n    return x.dayofweek\n\ntrain[\"year\"] = train.date_format.apply(get_year)\ntrain[\"month\"] = train.date_format.apply(get_month)\ntrain[\"day\"] = train.date_format.apply(get_day)\ntrain['dayofweek'] = train.date_format.apply(get_weekday)\n\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7611a03365669c1cf09b33cf37344d1c17ad7e99",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# First analyzing the shape of the two datasets\nprint(\"Description of training set\")\nprint(\"#Rows, #columns: \\t\\t\\t\", train.shape)\nprint(\"#NaNs: \\t\\t\\t\", train.isna().sum().sum()) \nprint(\"#Shops: \\t\\t\", train.shop_id.nunique())\nprint(\"#Item Categories: \\t\", train.item_category_id.nunique())\nprint(\"#Items: \\t\\t\", train.item_id.nunique())\nprint(\"#Months: \\t\\t\", train.date_block_num.nunique())\nprint(\"Description of test set\")\nprint(\"#Rows, #columns: \\t\\t\\t\", test.shape)\nprint(\"#Shops: \\t\\t\", test.shop_id.nunique())\nprint(\"#Items: \\t\\t\", test.item_id.nunique())\n\n# Checking whether all the shops and items are present in the two datasets\ntrain_items = train.item_id.unique()\ntest_items_not_in_train = test[~test.item_id.isin(train_items)].item_id.unique()\nprint('%d items in test data not found in train data' % len(test_items_not_in_train))\n\ntrain_shops = train.shop_id.unique()\ntest_shops_not_in_train = test[~test.shop_id.isin(train_shops)].shop_id.unique()\nprint('%d shops in test data not found in train data' % len(test_shops_not_in_train))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21f400443c7f4ad69eee604241bd353d328cb974"
      },
      "cell_type": "markdown",
      "source": "*  Above confirms that there are no NaNs in the train dataset\n* Also, while all the shops are captured in both test and train, some of the items in test aren't captured in train; therefore, we would need to create a comprehensive train dataset capturing all shop-item pairs"
    },
    {
      "metadata": {
        "_uuid": "c11c9cc57e4cb20c8f4b6d019f91622a2bcc96e3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#We are supposed to compute item counts at monthly-level; therefore, we roll-up item counts and revenue at a month level\nMonthlyRev = pd.DataFrame(train.groupby([\"month\", \"year\"], as_index=False)[\"total_sales\",\"item_cnt_day\"].sum())\nMonthlyRev.head()\nRevPlot = sns.FacetGrid(data = MonthlyRev.sort_values(by=\"month\"), hue = \"year\", height = 4, legend_out=True)\nRevPlot_1 = RevPlot.map(plt.plot, \"month\", \"total_sales\")\nRevPlot_1.add_legend()\nRevPlot_1;\nCountPlot = sns.FacetGrid(data = MonthlyRev.sort_values(by=\"month\"), hue = \"year\", height = 4, legend_out=True)\nCountPlot = CountPlot.map(plt.plot, \"month\", \"item_cnt_day\")\nCountPlot.add_legend()\nCountPlot;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8fd474dcc7327cbcc8066cb59771ecb9df87dd7f"
      },
      "cell_type": "markdown",
      "source": "Reviewing the monthly count and sales plot we observe the following:\n* The revenue and the count plot seems to suggest that 2015 sales are lower than 2013 and 2014 sales. Also, the sales seem to spike in Nov-Dec period in the previous years\n* Also, considering that the avg count in 2015 seems considerably lesser than avg count in 2013, 2014; however, the differnce in revenue in the same time period isn't as pronounced. It seems to suggest that price too has an effect and varies across time"
    },
    {
      "metadata": {
        "_uuid": "2d82516ee03aa77e9271ab16c7dc900a08dc6b81",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Further, we drill-down to observe trends at a weekly-level\nWeeklyRevenue = pd.DataFrame(train.groupby([\"dayofweek\"], as_index=False)[\"total_sales\"].sum())\nsns.barplot(x='dayofweek', y='total_sales', data=WeeklyRevenue)\n#  Monday-Sunday=0 to 6",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "908dd34e5c9114826e7430303a60afaee23d906a"
      },
      "cell_type": "markdown",
      "source": "* Drilling down to the weekly-level we observe that the sales seem to particularly spike on the weekends (Fri-Sun) and perhaps this can be used as a feature"
    },
    {
      "metadata": {
        "_uuid": "36f23922466cd2e3c9aaece15213c8d3eff2e1be",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# By reviewing item_category level sales we observe that some of the item categories are clear winners with respect to sales/ revenue\nMonthlyRev_Cat = pd.DataFrame(train.groupby([\"month\", \"year\",\"item_category_id\",\"item_category_name\"], as_index=False)[\"total_sales\",\"item_cnt_day\"].sum())\nMonthlyRev_Cat.nlargest(10, 'total_sales')\nMonthlyRev_Cat.nlargest(10, 'item_cnt_day')\n# The highest revenue generating category seems to be games/ gaming consoles while the item with the most sales seems to be Cinema DVDs\n# Therefore, we can say that some of the item categories seem to have more sales than others",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ea9c02be7278d67131008f61836055f4d62448e0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# By reviewing shop item_category level sales we observe that some of the item categories are clear winners with respect to sales/ revenue\nMonthlyRev_ShopCat = pd.DataFrame(train.groupby([\"month\", \"year\",\"shop_id\",\"item_category_id\",\"item_category_name\"], as_index=False)[\"total_sales\",\"item_cnt_day\"].sum())\nMonthlyRev_ShopCat.nlargest(10, 'total_sales')\nMonthlyRev_ShopCat.nlargest(10, 'item_cnt_day')\n# We can say that some of the shop item categories combinations seem to have more sales than others",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "abf236b6296c977211f1aff888959aa1b624448a"
      },
      "cell_type": "markdown",
      "source": "* Per the above two item-category and shop-item category level sales, we observe that some of the categories/ shops/ items are consistently showing high sales\n* Also, one more thing to note is that if one reviews the item category name it seems that the first word captures the item-master category as it is the same across categories"
    },
    {
      "metadata": {
        "_uuid": "8ba2625abd1176b3422a2bfe5b18789c3e47a1b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Checking for outliers with respect to item price\ntrain.item_price.plot()\ntrain[train['item_price'] > 100000]\n# There is a clear outlier in terms of item_price Radmin 3 which has only one record, so perhaps this can be removed",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9d1b2be263d661c5bfc3bd5997cb7902af94dd2c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Checking for outliers with respect to item count\ntrain.item_cnt_day.plot()\ntrain[train['item_cnt_day'] > 900]\n# There are a few outliers in terms of item counts so these would need to be handled",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "850c5b0dbc5424fef809f5036c9399c559f086e5"
      },
      "cell_type": "markdown",
      "source": "* By visual inspection of the price and count plots, we have observed a few outliers above and we would need to handle these cases as a part of data cleaning"
    },
    {
      "metadata": {
        "_uuid": "26c763b48e0df858448ca8b232dd8a2ae96e8f65",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Given that the data is a time-series, we should analyze it and check for stationarity\n# Using the Augmented Dicky Fuller Test (ADF) we would test the stationarity of our data\n# Confirming the stationarity of data would tell if regular statistical models can be used on the dataset\nts=train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts=ts.astype('float')\nfrom statsmodels.tsa.stattools import adfuller\nimport warnings\nwarnings.filterwarnings('ignore')\ndef test_stationarity(timeseries):\n    print('ADF Test Result:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(ts)\n\n# The P-value here is out of the expected range; therefore, we would de-seasonalize our dataset and rerun the stationarity test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bdddb332c6f0444c52dd92ab7564304e5a92e35f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Remove seasonal trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob\n\n# Plot total sales as original, after de-trending, and after de-seasonalization\nts=train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts=ts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "79dcf24e1f8c302374025189c9562d648ddcb1f9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Rerun stationarity test after de-seasonality \ntest_stationarity(new_ts)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bd7c179d58a79a6a29cc1a4d9bc3137b8e10be6"
      },
      "cell_type": "markdown",
      "source": "* Since the p-value is within 5%, we can assume our sales training data has stationarity and proceed with using standard statistical modeling methods\n* We shall be attempting to transform the given time-series problem into a problem which can be solved using standard statistical modelling (regression) by the sliding window method"
    },
    {
      "metadata": {
        "_uuid": "dc24e97831f2335944b10aabddcaa3ad0244e52a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Delete all dataframes created and release memory\nalldfs = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]\ndel [MonthlyRev_Cat,MonthlyRev_ShopCat,WeeklyRevenue,item_cats,items,shops,test,train]\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "409a914f9fd62e3a2af2a6eda44dd5e8e5632286"
      },
      "cell_type": "code",
      "source": "alldfs = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]\nalldfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13fbddb7955e3950a75bcfdde6865a63022721d0"
      },
      "cell_type": "markdown",
      "source": "**Data Processing and Feature Engineering** - Here is a summary of the data processing steps and feature engineering incorporated in the data"
    },
    {
      "metadata": {
        "_uuid": "f972f1f8e429ddfd62d6437c720e779c8a72d37b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Re-import all the data files (as we cleared the memory in the above step)\ntrain = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nitem_cats = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n#An Excel Input used for easy import of number of Fridays, Saturdays and Sundays in each month\ncalendar = pd.read_csv(\"../input/calendar/Calendar.csv\")\n\ncalendar.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9cc2f919e07a3a35d3318d2b73cf4a6a83d92890",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# As it was observed that not all shop-item combinations are present in train, we'll make a combined dataset with all shop-item-date combinations\nindex_col = ['shop_id', 'item_id', 'date_block_num']\nall_combi = []\nfor block in train['date_block_num'].unique():\n    shops_list = train.loc[train['date_block_num'] == block, 'shop_id'].unique()\n    items_list = train.loc[train['date_block_num'] == block, 'item_id'].unique()\n    all_combi.append(np.array(list(product(*[shops_list, items_list, [block]])),dtype='int32'))\nall_combi = pd.DataFrame(np.vstack(all_combi), columns = index_col,dtype=np.int32)\n\n#Remove outliers from the dataset\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<=900]\n\n#Calculate Monthly-Sales Train data as all computation is required to be done at monthly level \nsales_data = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()\n\n# Bring in the totals computed above against the all shop-item combination dataset\nsales_data = pd.merge(all_combi,sales_data,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ff7491701ba0ed5087a6567b39420e5a0789992",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Group the item categories into higher-level ones as we observed that the first word in the item category column indicates a master category\nl_cat = list(item_cats.item_category_name)\nfor ind in range(0,1):\n     l_cat[ind] = 'PC Headsets / Headphones'\nfor ind in range(1,8):\n     l_cat[ind] = 'Accessories'\nl_cat[8] = 'Tickets'\nl_cat[9] = 'Delivery of goods'\nfor ind in range(10,18):\n     l_cat[ind] = 'Consoles'\nfor ind in range(18,25):\n     l_cat[ind] = 'Console Games'\nl_cat[25] = 'Accessories for Games'\nfor ind in range(26,28):\n     l_cat[ind] = 'Phone Games'\nfor ind in range(28,32):\n     l_cat[ind] = 'CD Games'\nfor ind in range(32,37):\n     l_cat[ind] = 'Cards'\nfor ind in range(37,43):\n     l_cat[ind] = 'Movies'\nfor ind in range(43,55):\n     l_cat[ind] = 'Books'\nfor ind in range(55,61):\n     l_cat[ind] = 'Music'\nfor ind in range(61,73):\n     l_cat[ind] = 'Gifts'\nfor ind in range(73,79):\n     l_cat[ind] = 'Programs'\nfor ind in range(79,81):\n     l_cat[ind] = 'Services'\nfor ind in range(81,83):\n     l_cat[ind] = 'Clean Media'\nl_cat[83] = 'Batteries'\n\nlb = preprocessing.LabelEncoder()\n\nitem_cats['cat_type'] = lb.fit_transform(l_cat)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9258cd1f739781b230bbeaa1059de6afaeab9750",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Bring in item related information\nsales_data = pd.merge(sales_data,items,on=['item_id'],how='left')\n\n#Bring in item category related information\nsales_data = pd.merge(sales_data,item_cats, on=['item_category_id'], how='left')\n\nsales_data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4dba163abfe8fbe2d2f332f339b4c7dc44bbd68d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create features related to item-month, shop-month,and item category-month for item price and count. These features will be later used to create lag variables\nfor feat_type in ['item_id','shop_id','item_category_id','cat_type']:\n    for column, func_desc, funcname in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n\n        featdf = sales_data.groupby([feat_type,'date_block_num']).agg(func_desc).reset_index()[[column,feat_type,'date_block_num']]\n        featdf.columns = [feat_type+'_'+funcname+'_'+column, feat_type,'date_block_num']\n        \n        sales_data = pd.merge(sales_data,featdf,on=['date_block_num', feat_type],how='left')\n\n#Similar to the above add shop-item category-month related feature which would be later used as a lag variable\nfor column, func_desc, funcname in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n    \n    featdf = sales_data.groupby(['shop_id','item_category_id','date_block_num']).agg(func_desc).reset_index()[[column,'shop_id','item_category_id','date_block_num']]\n    featdf.columns = ['shop_itemcat'+'_'+funcname+'_'+column, 'shop_id','item_category_id','date_block_num']\n    \n    sales_data = pd.merge(sales_data,featdf,on=['date_block_num','shop_id','item_category_id'],how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a01e4f08b40bbcdd4f2fdbcfaecc39a24f5ad818"
      },
      "cell_type": "code",
      "source": "# Expanding Mean-encoding for target variable (Rationale for using this technique is explained after a few snippets) \n#This feature would be later used as a lag variable\nTarget = 'item_cnt_day'\nglobal_mean =  sales_data[Target].mean() # global mean value used for imputation in case of NaNs\ny_tr = sales_data[Target].values\n\nmean_encoded_col = ['shop_id', 'item_id', 'item_category_id', 'cat_type']\n\nfor col in tqdm(mean_encoded_col):\n    col_tr = sales_data[[col] + [Target]]\n    corrcoefs = pd.DataFrame(columns = ['Cor'])\n\n    cumsum = col_tr.groupby(col)[Target].cumsum() - col_tr[Target]\n\n    sumcnt = col_tr.groupby(col).cumcount()\n    col_tr[col + '_cnt_day_mean_Expanding'] = cumsum / sumcnt\n    col_tr[col + '_cnt_day_mean_Expanding'].fillna(global_mean, inplace=True)\n\n    corrcoefs.loc[col + '_cnt_day_mean_Expanding'] = np.corrcoef(y_tr, col_tr[col + '_cnt_day_mean_Expanding'])[0][1]\n\n    sales_data = pd.concat([sales_data, col_tr[corrcoefs['Cor'].idxmax()]], axis = 1)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "399f23c13903eb157fb22227d3b57dceb27f5d1f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Create test data and append it with sales data (train)\ntemp_test = test.copy()\ntemp_test['date_block_num'] = 34\ntemp_test.drop('ID', axis=1, inplace=True)\n\ntemp_test = temp_test.merge(items, how='left', on='item_id')\ntemp_test = temp_test.merge(item_cats, how='left', on='item_category_id')\ntemp_test.drop('item_name', axis=1, inplace=True)\n\nsales_data.drop('item_name', axis=1, inplace=True)\nsales_final = pd.concat([sales_data,temp_test], axis=0, ignore_index=True)\n\nsales_final.shape\nsales_final.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "eaa2eb8227a8da8bb45ec1965569fc5bc9dad4aa"
      },
      "cell_type": "code",
      "source": "#Delete dataframes and clear memory\ndel [featdf,all_combi,col_tr,corrcoefs,temp_test]\nalldfs = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]\nalldfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d8e38da73a573a378ff2182cbc62f3745e88546b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Had previously added simple mean-encoded features for item, shop, category and item-shop combinations\n#However, it was observed that due to simple mean encoding some amount of overfitting was happening. Therefore, mean encoding with regularization (Expanding mean-encoding) was implemented instead\n#for type_ids in [['item_id'], ['shop_id'], ['cat_type'], ['item_id', 'shop_id']]:\n#    for column_id in ['item_price', 'item_cnt_day']:\n#        mean_df = sales_data[type_ids + [column_id]].groupby(type_ids).agg(np.mean).reset_index()\n#        mean_df.rename(\n#            {column_id: \"mean_of_\"+column_id+\"_groupby_\"+\"_\".join(type_ids)},\n#            axis='columns', inplace=True\n#        )\n#        sales_final = pd.merge(sales_final, mean_df, on=type_ids, how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7cc61c8c8b948e8f87a94d75c2d006b0a52ccb45",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Create lag features for selected months\nlag_variables  = ['item_id_avg_item_price',\n'item_id_sum_item_cnt_day',\n'item_id_avg_item_cnt_day',\n'cat_type_avg_item_cnt_day',\n'cat_type_avg_item_price',\n'cat_type_sum_item_cnt_day',\n'shop_id_avg_item_price',\n'shop_id_sum_item_cnt_day',\n'shop_id_avg_item_cnt_day',\n'shop_itemcat_avg_item_cnt_day',\n'shop_itemcat_avg_item_price',\n'shop_itemcat_sum_item_cnt_day',\n'item_category_id_avg_item_price',\n'item_category_id_sum_item_cnt_day',\n'item_category_id_avg_item_cnt_day',\n'shop_id_cnt_day_mean_Expanding',\n'item_id_cnt_day_mean_Expanding',\n'item_category_id_cnt_day_mean_Expanding',\n'cat_type_cnt_day_mean_Expanding',                  \n'item_cnt_day']\n\n#Declare months for which lag features need to be created\nlags = [1,2,3,4,5,6,12]\n\nfor lag in lags:\n    sales_new_df = sales_final.copy()\n    sales_new_df.date_block_num+=lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    sales_final = pd.merge(sales_final,sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')\n\n\n# Since the latest month from which lags are available is 12 drop the data prior to it\nsales_final = sales_final[sales_final['date_block_num']>12]\n\n#fill missing values\nfor feat in sales_final.columns:\n    if 'item_cnt' in feat:\n        sales_final[feat]=sales_final[feat].fillna(0)\n    elif 'item_price' in feat:\n        sales_final[feat]=sales_final[feat].fillna(sales_final[feat].median())\n\n# Bring in the days related features from the calendar file\nsales_final = pd.merge(sales_final,calendar[['date_block_num','MonthNo','Fridays','Saturdays','Sundays','NormalDays','TotalDays']],on='date_block_num', how='left')\n\n#Drop extra columns\ncols_to_drop = lag_variables[:-1] + ['item_price']\nsales_final = sales_final.drop(cols_to_drop, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "98a0d196ad7a98a7c2edf7d1e03a101389c19fbb"
      },
      "cell_type": "code",
      "source": "# Create delta lag features for month 1 vs. month 2 for all the item, item category and shop-item category related features created above (except the mean-encoding ones)\nnumeric_cols = lag_variables[:-5]\nfor cols in numeric_cols: \n    newName = cols + '_1m_diff'\n    sales_final[newName] = sales_final[cols + '_lag_1'] - sales_final[cols + '_lag_2']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e67cdaf3a66c003463643319bfa893a6ff93941e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# The plan is to use holdout scheme for cross validation\n# 12 to 32 month data would be used for training\n# 33 month data would be used for CV\n# 34 month would be the test month\n\nX_train = sales_final[sales_final['date_block_num']<33]\nX_cv =  sales_final[(sales_final['date_block_num']==33)]\nX_test = sales_final[sales_final['date_block_num']==34]\n\n#Values are clipped at 20 considering the distribution of the target variable\nX_train['item_cnt_day'].clip_upper(20, inplace=True)\nX_train['item_cnt_day'].clip_lower(0, inplace=True)\n\nX_cv['item_cnt_day'].clip_upper(20, inplace=True)\nX_cv['item_cnt_day'].clip_lower(0, inplace=True)\n\nX_test['item_cnt_day'].clip_upper(20, inplace=True)\nX_test['item_cnt_day'].clip_lower(0, inplace=True)\n\n#Commented out:Write the dataframes into memory\n#X_train.to_csv('X_train.csv', index=False)\n#X_cv.to_csv('X_cv.csv', index=False)\n#X_test.to_csv('X_test.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c7f23bcebdf084775d66fa12d2f960668b9ee20a"
      },
      "cell_type": "markdown",
      "source": "**Model 1** - Simple Linear Regression (LB - 1.15) *(Commented out)*\nThe idea was to first try out a simple regression model and see how it was fairing on the LB and then try further complex models"
    },
    {
      "metadata": {
        "_uuid": "078701c53a46d6dfd86e85619d3745ed35ce9937",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# First trying a simple linear regression model on the dataset\n# all_data = sales_final\n# dates = all_data['date_block_num']\n# dates_train  = dates[dates <  34]\n# dates_test  = dates[dates == 34]\n\n# target_range = [0, 20]\n# cols = [c for c in X_train.columns if c not in ['date_block_num', 'item_cnt_day','item_category_name']]\n# colsa = [c for c in X_train.columns if c not in ['item_category_name']]\n\n# all_data = all_data[colsa]\n\n# X_train_1 = all_data[all_data['date_block_num']<34]\n# X_test_1 = all_data[all_data['date_block_num']==34]\n\n# X_train = X_train_1[cols]\n# X_test = X_test_1[cols]\n\n# y_train = X_train_1['item_cnt_day']\n# y_test =  X_test_1['item_cnt_day']\n\n# lr = LinearRegression()\n# lr.fit(X_train.values, y_train)\n\n# pred_lr = lr.predict(X_test.values).clip(*target_range)\n\n# submission = pd.DataFrame({\n#    \"ID\": test.index, \n#    \"item_cnt_month\": pred_lr\n# })\n# submission.to_csv('lr_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "daebd0a2fa50259e4107fca0ff56f7074ead2a11"
      },
      "cell_type": "markdown",
      "source": "**Model 2 -** LGBM (LB = 1.1) -* (Commented out)*\n* The next attempt was to try LGBM model and some parameters were tried (using hit and trial); however, as the training/ CV was taking a lot of time and the results didn't seem exceptionally better than simple linear regression\n* The plan was also to use a linear regression and LGBM ensemble; however,  the results from a quick simple convex mix solution (LB = 1.05) weren't a significant improvement\n* Therefore, I decided to go for an XGBoost solution"
    },
    {
      "metadata": {
        "_uuid": "1c8278eb2f59ab384874b6dbe1e19e87cbc4a919",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# X_train_1 = X_train.drop(['item_cnt_day','item_category_name'], axis=1)\n# Y_train = X_train['item_cnt_day']\n# X_valid_1 = X_cv.drop(['item_cnt_day','item_category_name'], axis=1)\n# Y_valid = X_cv['item_cnt_day']\n# X_test = X_test.drop(['item_category_name'], axis=1)\n\n# train_data = lgb.Dataset(data=X_train_1, label=Y_train)\n# valid_data = lgb.Dataset(data=X_valid_1, label=Y_valid)\n\n# params = {\n#               'feature_fraction': 0.75,\n#               'metric': 'rmse',\n#               'nthread':1, \n#               'min_data_in_leaf': 2**7, \n#               'bagging_fraction': 0.75, \n#               'learning_rate': 0.03, \n#               'objective': 'mse', \n#               'bagging_seed': 2**7, \n#               'num_leaves': 2**7,\n#               'bagging_freq':1,\n#               'verbose':0 \n#              }\n#    \n# lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=1000) \n# Y_test = lgb_model.predict(X_test).clip(0, 20)\n\n# submission = pd.DataFrame({\n#    \"ID\": test.index, \n#    \"item_cnt_month\": Y_test\n# })\n# submission.to_csv('lgbm_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "044b84d2dd100bfe41a6d6e65959e970d92494fb"
      },
      "cell_type": "markdown",
      "source": "**Model 3**  - XGBoost (LB = 0.90661)\n*   Per the points stated above, I decided to implement XGBoost for the problem. My decision was also motivated by the fact that XGBoost is the most widely used algorithm in Kaggle challenges\n* The parameters for XGBoost were optimized using RandomizedSearchCV over an native XGBoost API/ GridSearchCV implemenation as they were too time consuming\n"
    },
    {
      "metadata": {
        "_uuid": "5682c858fda9e3239ee216a73404269154b77cf8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Commented out as it takes considerable time to complete all iterations\n# Hyperparameter optimization using RandomizedSearchCV\n# from sklearn.model_selection import RandomizedSearchCV\n# from sklearn.metrics import mean_squared_error\n\n#X_train_1 = X_train\n#X_cv = X_cv\n#X_test_1 =X_test\n\n#cols = [c for c in X_train_1.columns if c not in ['date_block_num', 'item_cnt_day','item_category_name']]\n\n#x_train = X_train_1[cols]\n#y_train = X_train_1['item_cnt_day']\n#x_valid = X_cv[cols]\n#y_valid = X_cv['item_cnt_day']\n#x_test = X_test_1[cols]\n\n#clf = xgb.XGBRegressor()\n\n#param_grid = {\n#        'silent': [False],\n#        'max_depth': [6, 10, 15, 20],\n#        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n#        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n#        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n#        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n#        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n#        'gamma': [0, 0.25, 0.5, 1.0],\n#        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n#        'n_estimators': [100]}\n#\n#fit_params = {'eval_metric': 'rmse',\n#              'early_stopping_rounds': 10,\n#              'eval_set': [(x_valid, y_valid)]}\n#\n#rs_clf = RandomizedSearchCV(clf, param_grid, n_iter=20,\n#                            n_jobs=1, verbose=2, cv=2,\n#                            fit_params=fit_params,\n#                            scoring='neg_mean_squared_error',random_state=42)\n#\n#rs_clf.fit(x_train, y_train)\n#\n#best_score = rs_clf.best_score_\n#best_params = rs_clf.best_params_\n#print(\"Best score: {}\".format(best_score))\n#print(\"Best params: \")\n#for param_name in sorted(best_params.keys()):\n#    print('%s: %r' % (param_name, best_params[param_name]))   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3caabcce8016b68bd49eeb61c4a712069abeb51e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# XGBoost Model Training and Fit\nparams = {\n'colsample_bytree': 0.9, 'eta': 0.2, 'eval_metric': 'rmse', 'gamma': 1.0,'lambda': 50.0,\n'max_depth': 8, 'min_child_weight': 300.0, 'n_estimators': 1000.0, 'objective': 'reg:linear', \n'seed': '50', 'silent': 1, 'subsample': 0.7, 'tree_method': 'exact', 'colsample_bylevel':0.4 \n}\n\ncols = [c for c in X_train.columns if c not in ['date_block_num', 'item_cnt_day','item_category_name']]\n\nx1 = X_train[cols]\ny1 = X_train['item_cnt_day']\nx2 = X_cv[cols]\ny2 = X_cv['item_cnt_day']\nwatchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\nmodel = xgb.train(params, xgb.DMatrix(x1, y1), 3500,  watchlist, maximize=False, verbose_eval=50, early_stopping_rounds=50)\n\npred = model.predict(xgb.DMatrix(X_test[cols]), ntree_limit=model.best_ntree_limit)\n\ntest['item_cnt_month'] = pred.clip(0,20)\ntest.drop(['shop_id', 'item_id'], axis=1, inplace=True)\ntest.to_csv('xgb_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "83dc658706c98e5404f27f0f0dccb4d2cec746f2"
      },
      "cell_type": "code",
      "source": "# Plot the feature importance\nfrom xgboost import plot_importance\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\nplot_features(model, (10,20))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "e0b967a879463057486b2ee04f004ea282d36a69"
      },
      "cell_type": "markdown",
      "source": "* Lags 1-3 seem to be the most important lag features suggesting a relation between recent month sales on future sales\n* Item count aggregates and lag features seem to be the most important features\n* Also, item id and item category id are important features which is in-line with the observation of some categories/ items showing high sales consistently\n* Item count delta price lag is also one of the important features suggesting that previous dip/ increase in sales impact future sales\n* As expected month number is an important feature in-line with the seasonal nature of the data\n* The feature capturing number of weekends/ weekdays isn’t an important feature; therefore, the peak in sales during weekends may not be generalizable",
      "attachments": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
